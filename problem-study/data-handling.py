# -*- coding: utf-8 -*-
"""data-handling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ctWeVb-b3u0u3c3lEb6WfhqqJ3XgfCwU

# Setup
"""

!pip install dython
import pandas as pd
import numpy as np
from google.colab import drive
from sklearn import datasets
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix 
from sklearn.model_selection import train_test_split 
from plotnine import *
from math import pi
import matplotlib.pyplot as plt
from sklearn import preprocessing
import dython
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""# Data Cleaning

Load datasets into notebook
"""

df=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/spotify-playlist-recommender/playlists.csv")
df

"""### Check for imabalances in our data"""

# We want to see the dritibution of our playlists
plt.figure(figsize=(10,10))

sns.barplot(x=df.Playlist.value_counts().index,
            y=df.Playlist.value_counts()).set(xticklabels=[])
plt.show()

# As we can see there are imbalances in our data
mean_song_count = int(df.Playlist.value_counts().mean())
max_song_count = int(df.Playlist.value_counts().max())
min_song_count = int(df.Playlist.value_counts().min())
std_songs = int(df.Playlist.value_counts().std())
print(f'Mean: {mean_song_count} songs, Max: {max_song_count} songs, Min: {min_song_count} songs, Std: {std_songs}')

# We want to see the playlist that create imbalances in our data
plt.figure(figsize=(10,10))
sns.barplot(x=df.Playlist.value_counts().loc[lambda count: count>mean_song_count].index,
            y=df.Playlist.value_counts().loc[lambda count: count>mean_song_count]).set(xticklabels=[])
plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0)
plt.show()
df.Playlist.value_counts().loc[lambda count: count>mean_song_count].index.tolist()

# Lets get rid of the excesive songs that create imbalances in our data
# We can see above that the playlists creating imbalances are: Janngueo, Rap(TLob), EDM(TLob), RapEspanol(TLob),...
# Downsampling (mean is 168 songs/playlist)
imbalanced_playlists = df.Playlist.value_counts().loc[lambda count: count>mean_song_count].index.tolist()

for playlist in imbalanced_playlists:
    songs_removed = 0
    playlist_songs = df.loc[df['Playlist'] == playlist]
    num_songs_to_remove = abs(mean_song_count-len(playlist_songs))
    for index,row in playlist_songs.iterrows():
      songs_removed = songs_removed + 1
      df.drop(df[(df['Playlist'] == row['Playlist']) & (df['Name'] == row['Name'])].index, inplace = True)
      if songs_removed == num_songs_to_remove:
        break

# Lets see the new distribution after balancing our data
plt.figure(figsize=(10,10))
sns.barplot(x=df.Playlist.value_counts().index,
            y=df.Playlist.value_counts()).set(xticklabels=[])
plt.show()

# As we can see there are imbalances in our data
mean_song_count = int(df.Playlist.value_counts().mean())
max_song_count = int(df.Playlist.value_counts().max())
min_song_count = int(df.Playlist.value_counts().min())
std_songs = int(df.Playlist.value_counts().std())
print(f'Mean: {mean_song_count} songs, Max: {max_song_count} songs, Min: {min_song_count} songs, Std: {std_songs}')

"""### Check for Missing values"""

df[df.isnull().any(axis=1)]

"""Let's see which playlists have missing values and how much percentage of the each playlist has missing values"""

playlists_percentage_nan = df.Playlist.loc[df.isnull().any(axis=1)].value_counts().to_dict()
for key, value in playlists_percentage_nan.items():
  playlists_percentage_nan[key] = int(100*(value/len(df.loc[df['Playlist'] == key])))

plt.figure(figsize=(10,10))
plt.bar(playlists_percentage_nan.keys(), playlists_percentage_nan.values())
ax = plt.gca()
ax.axes.xaxis.set_visible(False)
plt.show()

"""We can safely remove the missing values from the playlists since we will never be lossing a whole playlist. In the worst case we loose half a playlist but we will take this as acceptable"""

df.dropna(inplace=True)
df['Playlist'].unique()

"""Lets see how is our distribution of songs per playlists after removing missing values"""

plt.figure(figsize=(10,10))
sns.barplot(x=df.Playlist.value_counts().index,
            y=df.Playlist.value_counts()).set(xticklabels=[])
plt.show()

mean_song_count = int(df.Playlist.value_counts().mean())
max_song_count = int(df.Playlist.value_counts().max())
min_song_count = int(df.Playlist.value_counts().min())
std_songs = int(df.Playlist.value_counts().std())
print(f'Mean: {mean_song_count} songs, Max: {max_song_count} songs, Min: {min_song_count} songs, Std: {std_songs}')

"""### Restructure our data columns"""

# Break artists into different songs:
  # Name     Artists                  Playlist AudioFeatures       ->            Name     Artists Playlist AudioFeatures
  # Safaera  BadBunny, Anuel, Mozart hOLA     X                                 Safaera  BadBunny hOLA     X
  #                                                                             ""        Anuel   ""        ""
  #                                                                             ""        Mozart  ""        ""
df_multiple_artist = df.copy()
df_multiple_artist['Artists'] = df_multiple_artist['Artists'].str.split(',')
df_multiple_artist = df_multiple_artist.explode('Artists').reset_index(drop=True)
df_multiple_artist

## We might to instead of exploding a song into different artists, just keep the song with the first artist in the aritsts list
#        This is because in training we dont want to give more importance to playlists (classes) which have songs with a lot of collabs
#        This will make the model bias towards the playlists with most collabs in it, instead of generalising
df_one_artist = df
df_one_artist['Artists'] = df_one_artist['Artists'].str.split(',')
for idx, row in df_one_artist.iterrows():
  df_one_artist.at[idx,'Artists'] = row['Artists'][0]
df_one_artist

# Renaming 
def rename(dataframe):
  dataframe.rename(columns={'Artists':'Artist'}, inplace=True )
  dataframe.rename(columns={'Duration(ms)':'Duration'}, inplace=True)
  dataframe.rename(index={'Dinner Track':'Dinner Tracks'}, inplace=True)
  dataframe.rename(index={'Party Track':'Party Trakcs'}, inplace=True)
  dataframe.rename(index={'Sleep Track':'Sleep Tracks'}, inplace=True)
  dataframe.rename(index={'Workout Track':'Workout Tracks'}, inplace=True)
rename(df_multiple_artist)
rename(df_one_artist)

"""Now, we save our clean dataset in order to use it in our models"""

df_one_artist.to_csv("/content/drive/MyDrive/Colab Notebooks/spotify-playlist-recommender/clean-playlists-one-artist.csv", index=False)
df_one_artist

df_multiple_artist.to_csv("/content/drive/MyDrive/Colab Notebooks/spotify-playlist-recommender/clean-playlists-multiple-artist.csv", index=False)
df_multiple_artist

"""# Data Study

**Using only one artist for each song**
"""

df=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/spotify-playlist-recommender/clean-playlists-one-artist.csv")
df.columns

"""**Or using multiple artist for each song, implying songs with more artists are have stronger impact on the model**"""

#df=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/spotify-playlist-recommender/clean-playlists-multiple-artist.csv")
#df

"""## Statistics

### Mean for all features
"""

df.describe()

print(df['Artist'].describe()) #TODO: Artists empty?
print(df['Album'].describe()) #TODO: Albums with 0.0??

"""### Correlation Matrix

#### Pearson's Corrleation Matrix
"""

plt.figure(figsize=(15,15))
sns.heatmap(df.corr(), annot=True)
plt.show()

c = df.corr().abs()
s = c.unstack()
so = s.sort_values(kind="quicksort", ascending=False)[15:]
so.head(n=10)

"""#### Categorial and Continuous Correlation Matrix
We use both Cramer's V and Theil's U for computing conditional entropys among values
"""

dython.nominal.associations(df, figsize=(15,15))

"""We can observe that **Artist, Explicit, Popularity, Acousticness, Danceability, Energy, Instrumentalness, Loudness and Valence** are the strongest correlated features with our **Playlist** feature

#### Conditional Entropies
"""

# Cond_entropy(y,x) = 0 -> y is completely determined by value of X -> the lower cond_entropy the better
cond_entropy = {}

for col1 in df.columns:
  cond_entropy[col1] = {}
  for col2 in df.columns:
    cond_entropy[col1][col2] = dython.nominal.conditional_entropy(df[col1],df[col2])


cond_entropy_df = pd.DataFrame.from_dict(cond_entropy, orient='index')
plt.figure(figsize=(15,15))
sns.heatmap(cond_entropy_df, annot=True)
plt.show()
# Verical is X , Y is horizontal
# i.e: The amount of information that I need to describe Name knowing Album is 0.33

"""We can observe that knowing song's **Name** and **Artist**, the **Album**, the **Loudness**, the **Duration** and the **Tempo** we need very little additional information to determine the playlist.

As we can observe from the plots the features that best allow us to determine the playlist label are:
* Energy, Instrumentalness, Loudness, Acousticness, Danceability, Popularity, Tempo, Duration, Name, Artist, Album

Therefore, we are going to get rid of all other features.

However we can also see there is strong correlations between:

* Energy - Loudness (0.77)
* Energy - Acousticness (0.75)
* Loudness - Acousticness (0.62)
* Loudness - Instrumentalness (0.55)

Hence, we are going to throw away Energy since we get the same info from Acousticness and Loudness. We choose to discard Energy over Loudness since Loudnes has correlation with Playlist of 0.83 and Energy 0.8

From the rest of the features we are unsure about keeping:
* Popularity, Name, Tempo, Artist, Duration and Album

Despite Popularity having strong correlation with Playlist, we can see from its conditional entropy that knowing it leaves us with a lot information left to determine the Playlist.

On the other hand, the rest of the features we are unsure about (Tempos, Name, Artist, Duration and Album) tell us a lot of information about the playlist as we see from their conditional entropies. However, they have kind of weak correlations with the playlist label.

To summarize, we keep at least for now:
* Instrumentalness, Loudness, Acousticness, Danceability, Popularity, Tempo, Duration, Name, Artist, Album
"""

df.groupby('Playlist').mean().reset_index()

"""## Analyzing doubtfull features

### Popularity
"""

continuous_columns = ['Duration', 'Popularity', 'Key', 'Mode', 'Time Signature', 'Acousticness', 'Explicit',
                      'Danceability',	'Energy',	'Instrumentalness',	'Liveness',	'Loudness',	'Speechiness', 'Valence', 'Tempo']
continuous_df = df[continuous_columns]
x = continuous_df.values 
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
scaled_df = pd.DataFrame(x_scaled, columns=continuous_columns)

#scaled_df = scaled_df.join(df['Playlist']).join(df['Name']).join(df['Artist']).join(df['Album'])
scaled_df['Playlist'] = df['Playlist'].values
scaled_df['Name'] = df['Name'].values
scaled_df['Album'] = df['Album'].values

scaled_df

mean_pl_df = scaled_df.groupby('Playlist').mean().reset_index()

scores ={}
for index, row in mean_pl_df.iterrows(): 
  scores[row['Playlist']] = row['Duration'] + row['Popularity'] + row['Key'] + row['Mode'] + row['Time Signature'] + \
                            row['Acousticness'] + row['Explicit'] + row['Danceability'] + row['Energy'] + row['Instrumentalness'] + \
                            row['Liveness'] + row['Loudness'] + row['Speechiness'] + row['Valence'] + row['Tempo'] 
  
 
scores_df = pd.DataFrame.from_dict(scores, orient='index', columns=['Score']).rename_axis('Playlist').reset_index()
print(scores_df.describe())

for index, row in scaled_df.iterrows(): 
  score = scores_df.loc[scores_df['Playlist'] == row['Playlist']]['Score'].iloc[0]
  scaled_df.at[index,'Score'] = score

scaled_df = scaled_df.groupby('Playlist').mean().reset_index()
scaled_df

(
  ggplot(scaled_df, aes(x='Popularity', y='Score', color='Playlist', fill='Playlist'))
    + geom_point(alpha=1)
)
#df['Popularity'].describe()

"""It is unclear whether Popularity is relevant or not, so we will keep it as for now

### Tempo and Duration

From these plots we can observe Tempo and Duration are actually useless to classify a song since they all have very similar Tempo and Duration. We discard Tempo and Duration from our feature set.
"""

(
    ggplot(df, aes(x='Tempo', color='Playlist', fill='Playlist'))
    + geom_density(alpha=0.1)
)

(
    ggplot(df, aes(x='Duration', color='Playlist', fill='Playlist'))
    + geom_density(alpha=0.1) + xlim(0, 500000)
)

"""### Name, Artist, Album"""

artist_album_count={}
albums = {}
artists = {}
for pl in df['Playlist'].unique():
  albums[pl] = set()
  artists[pl] = set()


for index, row in df.iterrows():
    albums[row['Playlist']].add(row['Album'])
    artists[row['Playlist']].add(row['Artist'])

for pl in df['Playlist'].unique():
    artist_album_count[pl] = {}
    artist_album_count[pl]['Album'] = len(albums[pl])
    artist_album_count[pl]['Artist'] = len(artists[pl])

artist_album_count_df = pd.DataFrame.from_dict(artist_album_count, orient='index').rename_axis('Playlist').reset_index()
artist_album_count_df.describe()

(
    ggplot(artist_album_count_df, aes(x='Artist', y='Album',color='Playlist', fill='Playlist'))
    + geom_point(alpha=1)
)

"""We will straight away discard Name since its imposible to classify a song given just a name. We will also discard the Artist. Both Artist and Album are very correlated as we see above and they give similiar information. However Artist does have a stronger correlation with Playlist and also gives us a better conditional entropy when given a Playlist. Therefore we stick with Artist.

To conclude the features that we will work with from now on will be:
* **Acousticness, Danceability, Instrumentalness, Loudness, Popularity and Artist**

Great! We just redcued our feature space from 18 dimensions to 6

# Optimal number of classes
"""

from plotnine import *
import seaborn as sns

from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import f1_score,confusion_matrix
from sklearn.metrics import accuracy_score

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import RFECV

from sklearn.neighbors import KNeighborsClassifier

"""We will run three different classifiers (Naive Bayes, Random Forest, Knn). 
These will classify with n classes, being n the number of playlist. In  
this Cell an iteration from 1 class to 80 classes will be executed in order to see determine the best classifier and the appropiate number of playlist to use in our model.
"""

NB={}; RF={}; KNN={}
for n_pl in range(1,len(df.groupby("Playlist").mean())+1):
       
    #Sampling with n_pl different random playlists 
    Random=df.groupby("Playlist").mean().sample(n=n_pl)
    sampled_data=df.loc[df["Playlist"].isin(Random.index)]
    
    #Creating our Features
    X=sampled_data.iloc[:,9:-1]
    features=X.columns

    #Creating our Label 
    y=sampled_data.loc[:,["Playlist"]]

    #Standarize the features
    X_Standarized=StandardScaler().fit_transform(X)
    X_Standarized=pd.DataFrame(X_Standarized)
    X_Standarized.columns=features
    
    #Split your Data
    x_train, x_test, y_train, y_test = train_test_split(X_Standarized, y, test_size=0.1, random_state=1)

    #Naive Bayes Classifier 
    NBClassifier = GaussianNB()
    NBClassifier = NBClassifier.fit(x_train, y_train)

    #Random Forest Classifier
    RFClassifier= RandomForestClassifier(random_state=2)      
    RFClassifier= RFClassifier.fit(x_train,y_train)

    #Knn Classifier
    KNNClassifier = KNeighborsClassifier(n_neighbors=3)
    KNNClassifier.fit(x_train, y_train)

    #Storing Accuracies in dictionaries
    NB[n_pl] = accuracy_score(y_test, NBClassifier.predict(x_test))
    RF[n_pl] = accuracy_score(y_test, RFClassifier.predict(x_test))
    KNN[n_pl] = KNNClassifier.score(x_test,y_test)

Accuracy_df=pd.DataFrame(NB.items()).astype(float)
Accuracy_df.columns=["N_Playlists","Accuracy NB" ]
Accuracy_df["Accuracy RF"]=RF.values()
Accuracy_df["Accuracy KNN"]=KNN.values()
Accuracy_df.head(n=20)

(
    ggplot(Accuracy_df)+geom_line(aes(x="N_Playlists",y="Accuracy NB"), color="r")
    + geom_line(aes(x="N_Playlists",y="Accuracy RF"), color="b")
    + geom_line(aes(x="N_Playlists",y="Accuracy KNN"), color="y")
    + labs(x = "Number of Playlists", y= "Accuracy",color = "Legend")
    + scale_color_discrete(name = "Accuracies", labels = ["Accuracy NB", "Accuracy RF","Accuracy KNN"])
)
#BLUE   = RANDOM FOREST
#RED    = NAIVE BAYES
#YELLOW = KNN

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <iframe src="https://drive.google.com/file/d/1-BT-FkNC5WOjfWyGYREyHRbS-20wqWlp/preview" width="640" height="480"></iframe>
# <iframe src="https://drive.google.com/file/d/1-AbKorApKoKEPlb-jljrmm42C-NhEGyv/preview" width="640" height="480"></iframe>

"""### Choosing 10 classes

We will try to use 10 playlist which include differentiated songs among the chosen playlists. To do so, we will use a score function for each song:

score(x) = sum(normalized_continuous_features)

For then, be able to plot the distribution of all playlists and choose 10 of them.
"""

continuous_columns = ['Duration', 'Popularity', 'Key', 'Mode', 'Time Signature', 'Acousticness', 'Explicit',
                      'Danceability',	'Energy',	'Instrumentalness',	'Liveness',	'Loudness',	'Speechiness', 'Valence', 'Tempo']
continuous_df = df[continuous_columns]
x = continuous_df.values 
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
scaled_df = pd.DataFrame(x_scaled, columns=continuous_columns)

#scaled_df = scaled_df.join(df['Playlist']).join(df['Name']).join(df['Artist']).join(df['Album'])
scaled_df['Playlist'] = df['Playlist'].values
scaled_df['Name'] = df['Name'].values
scaled_df['Album'] = df['Album'].values

scaled_df

mean_pl_df = scaled_df.groupby('Playlist').mean().reset_index()

scores ={}
for index, row in mean_pl_df.iterrows(): 
  scores[row['Playlist']] = row['Duration'] + row['Popularity'] + row['Key'] + row['Mode'] + row['Time Signature'] + \
                            row['Acousticness'] + row['Explicit'] + row['Danceability'] + row['Energy'] + row['Instrumentalness'] + \
                            row['Liveness'] + row['Loudness'] + row['Speechiness'] + row['Valence'] + row['Tempo'] 
  
 
scores_df = pd.DataFrame.from_dict(scores, orient='index', columns=['Score']).rename_axis('Playlist').reset_index()
print(scores_df.describe())

for index, row in scaled_df.iterrows(): 
  score = scores_df.loc[scores_df['Playlist'] == row['Playlist']]['Score'].iloc[0]
  scaled_df.at[index,'Score'] = score

scaled_df = scaled_df.groupby('Playlist').mean().reset_index()
scores_df = scaled_df
scores_df

plot_1 = ggplot(scores_df.iloc[0:8], aes(x='Playlist', y='Score', color='Playlist', fill='Playlist')) + geom_point(alpha=1)
plot_2 = ggplot(scores_df.iloc[9:17], aes(x='Playlist', y='Score', color='Playlist', fill='Playlist')) + geom_point(alpha=1)
plot_3 = ggplot(scores_df.iloc[18:26], aes(x='Playlist', y='Score', color='Playlist', fill='Playlist')) + geom_point(alpha=1)
plot_4 = ggplot(scores_df.iloc[27:35], aes(x='Playlist', y='Score', color='Playlist', fill='Playlist')) + geom_point(alpha=1)
plot_5 = ggplot(scores_df.iloc[36:44], aes(x='Playlist', y='Score', color='Playlist', fill='Playlist')) + geom_point(alpha=1)
plot_6 = ggplot(scores_df.iloc[45:53], aes(x='Playlist', y='Score', color='Playlist', fill='Playlist')) + geom_point(alpha=1)
plot_7 = ggplot(scores_df.iloc[54:62], aes(x='Playlist', y='Score', color='Playlist', fill='Playlist')) + geom_point(alpha=1)
plot_8 = ggplot(scores_df.iloc[63:71], aes(x='Playlist', y='Score', color='Playlist', fill='Playlist')) + geom_point(alpha=1) 
plot_9 = ggplot(scores_df.iloc[72:80], aes(x='Playlist', y='Score', color='Playlist', fill='Playlist')) + geom_point(alpha=1) 
plot_10 = ggplot(scores_df.iloc[80:84], aes(x='Playlist', y='Score', color='Playlist', fill='Playlist')) + geom_point(alpha=1)

plot_1

plot_2

plot_3

plot_4

plot_5

plot_6

plot_7

plot_8

plot_9

plot_10

"""From all of these we select these ones sicne they are very far apart:
* **Tuff, Yoga, Punk Espanol, Rap Espanol(TLob), Metal, Janngueo, DubReggae, DailyMix3, DailyMix5, CountryNights**
"""

chosen_pls = ['Tuff', 'BlueBallads', 'PunkEspanol', 'RapEspanol(TLob)', 'Metal', 'Romanticism', 'Sleep Tracks', 'GoldSchool', 'Chill', 'CountryNights']
#chosen_pls = ['Tuff', 'Yoga', 'PunkEspanol', 'RapEspanol(TLob)', 'Metal', 'Janngueo', 'DubReggae', 'DailyMix3', 'DailyMix5', 'CountryNights'] Mart√≠n
#chosen_pls = ['PowerHour', 'BonfireSongs', 'Blues', 'Yoga', 'EDM', 'Espanola', 'Janngueo', 'LofiBeats', 'Metal', 'PunkUnleashed'] Jimmy
scores_10_pl_df = scores_df.loc[scores_df['Playlist'].isin(chosen_pls) ]
ggplot(scores_10_pl_df, aes(x='Playlist', y='Score', color='Playlist', fill='Playlist')) + geom_point(alpha=1)